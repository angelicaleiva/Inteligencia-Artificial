# -*- coding: utf-8 -*-
"""Trabajo_NLP.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Bzvt13bXoEsBM7LO2vJNVCC42BexDYh-

# Procesamiento de Lenguaje Natural - Universidad Adolfo Ibañez

Fecha: 28-09-2020

Profesor: John Atkinson

Alumnos:

*   Angélica Leiva
*   Jorge Navarro
*   Erick Villarroel

**Se importan las librerias a utilizar**"""

#!python -m spacy download es_core_news_sm
import os
import numpy as np
import es_core_news_sm
import regex
from sklearn.feature_extraction.text import TfidfVectorizer
from string import punctuation
import joblib
import pandas as pd
import nltk
import random
from random import randrange, randint
from nltk.tokenize import sent_tokenize, word_tokenize
from scipy.spatial.distance import cosine 
nltk.download('punkt')

"""**Setup Carpeta de Trabajo**"""

# Importar desde Google Drive
# from google.colab import drive
# drive.mount('/content/drive')
# path_discursos = '/content/drive/My Drive/NLP/discursos/'
# path_lsa = '/content/drive/My Drive/NLP/mi_lsa/'
# path_doc = '/content/drive/My Drive/NLP/'

# Carpetas PC Local
path_discursos = "C:/nlp/discursos/"
path_lsa = "C:/nlp/mi_lsa/"
path_doc = "C:/nlp/"

"""**Se crea el Espacio LSA y el Corpus**"""

def CrearEspacioLSA(corpus,numDim,NombreModelo):
  textos = PreProcesar(corpus)
  transf = TfidfVectorizer()
  tf = transf.fit_transform(textos).T
  U, Sigma, VT = np.linalg.svd(tf.toarray())
  terms = np.dot(U[:,:numDim], np.diag(Sigma[:numDim]))
  docs = np.dot(np.diag(Sigma[:numDim]), VT[:numDim, :]).T 
  vocab = transf.get_feature_names()
  GrabarModeloLSA(NombreModelo, Sigma, terms, docs, vocab)


def GrabarModeloLSA(NombreModelo,Sigma,terms,docs, vocab):
   existe = os.path.isdir(NombreModelo)
   if not existe:
       os.mkdir(NombreModelo)
   joblib.dump(Sigma,   NombreModelo +"/"+'sigma.pkl') 
   joblib.dump(terms,   NombreModelo +"/"+'terms.pkl') 
   joblib.dump(docs,    NombreModelo +"/"+'docs.pkl') 
   joblib.dump(vocab,   NombreModelo +"/"+'vocab.pkl') 

    
def CargarModeloLSA(NombreModelo):
    sigma   = joblib.load(NombreModelo+"/"+'sigma.pkl')
    terms   = joblib.load(NombreModelo+"/"+'terms.pkl')
    docs    = joblib.load(NombreModelo+"/"+'docs.pkl')
    vocab   = joblib.load(NombreModelo+"/"+'vocab.pkl')
    return(sigma, terms, docs, vocab)

def CrearDiccionario(Vectores,vocabulario):
   dicc = {}
   for  v in range(0,len(vocabulario)):
      dicc[vocabulario[v]] = Vectores[v]
   return(dicc)


def CrearCorpus(path):
  directorio = os.listdir(path)
  corpus = []
  for filename  in directorio:
     texto = open(path+filename,'r',encoding="utf-8").read()
     corpus.append(texto)
  return(corpus)

def PreProcesar(textos):
    texto_limpio = []
    for texto in textos:  
        texto = EliminaNumeroYPuntuacion(texto)      
        texto_limpio.append(texto)
    return(texto_limpio)

def EliminaNumeroYPuntuacion(oracion):
    string_numeros = regex.sub(r'[\.\”\“\¿\°\d+]','',oracion)
    return ''.join(c for c in string_numeros if c not in punctuation)


def Similitud(Vec1,Vec2):
    sim = 1 - cosine(Vec1, Vec2)
    return(sim)


# Modificar la ruta PATH con la ubicación de un corpus a utilizar
PATH = path_discursos

nlp    = es_core_news_sm.load()
NumDim =  10
corpus = CrearCorpus(PATH)

# CrearEspacioLSA(corpus,NumDim,path_lsa)

# El espacio LSA se crea una vez solamente (o se re-genera cada cierto tiempo)
# Realizar otras taeas en forma asincrónica

(Sigma, vect_terms, vect_docs, lista_palabras) = CargarModeloLSA(path_lsa)

terms = CrearDiccionario(vect_terms, lista_palabras)

"""**Revisión Corpus**"""

print('Cantidad de Discursos: ',len(corpus))

"""**Se separan los discursos en frases**"""

# Se separan los discursos en frases
textos = [sent_tokenize(i) for i in corpus]

# Se crea un DataFrame para almacenar el discurso tokenizado
frases = pd.DataFrame(textos)

# Se visualizan los 5 primeros discursos
frases.head(5)

"""**Se crea una función para separar oraciones en palabras y calcular el vector de la oración:**"""

# Calcula el vector para las oraciones
def calcula_vector2(oracion):
    palabras = word_tokenize(oracion.lower())
    i=total=promedio=0
    for palabra in palabras:
        if palabra in terms:
            i += 1
            total+=terms[palabra]
    if i>0:
        promedio = total/i
    return promedio

"""**Generar discurso**"""

# Número de frases del discurso
N = 50

#Elegir la primera frase de un discurso aleatorio
O = frases.iloc[random.randrange(0, len(frases)), 0]

#Se calcula el vector de la primera frase
vector1= calcula_vector2(O) 

#Se guarda la primera frase del discurso
Nuevo_texto = [O]

# print(Nuevo_texto)

# Se eligen las siguientes frases

# Inicialmente se consideró incluir la oración de mayor similitud de un documento aleatorio,
# Sin embargo, realizamos pruebas para buscar mayor coherencia del discurso con las nuevas oraciones,
# resultando mejor incorporar dentro del analisis de similitud todas las oraciones de todos los discursos,
# aunque requiere mayor tiempo para procesar el discurso, nuestro trabajo considero esta última opción
# de búsqueda de oraciones, es decir se seleccionó la oración con mejor similitud del Corpus completo.

n_columnas=len(frases.columns)

print('Oración: ',1,' de ',N,' ...')
for n in range(0,N-1):
  similitud=[]
  nueva_frase=[]
  vector=[]
  if n <= N-3:
    for discurso2 in range(0,len(frases)):
      for j in range(1,n_columnas):
          revisa_frase= frases.iloc[discurso2, j]
          if revisa_frase not in Nuevo_texto and revisa_frase is not None and revisa_frase.count('gracias')<=0:
            vector2= calcula_vector2(revisa_frase)
            similitud_j = Similitud(vector1,vector2)
            nueva_frase.append(revisa_frase)
            similitud.append(similitud_j)
            vector.append(vector2)
    frases_similitud = pd.Series(data=similitud, index=nueva_frase)
    frases_vector = pd.Series(data=vector, index=nueva_frase)
    max_sim = frases_similitud.idxmax()
    vector1 = frases_vector[frases_similitud.idxmax()]
    Nuevo_texto.append(max_sim)
    del similitud_j, nueva_frase
    print('Oración: ',n+2,' de ',N,' ...')
  else: # Sólo si es la última oración, selecciona frase con mayor similitud de todos los documentos (mejor frase de cierre) 
    for discurso2 in range(0,len(frases)):
      nueva_frase2=frases[discurso2:discurso2+1].dropna(axis=1,how='all')
      max_columnas = len(nueva_frase2.columns)
      revisa_frase= frases.iloc[discurso2, max_columnas-1]
      vector2= calcula_vector2(revisa_frase)
      similitud_j = Similitud(vector1,vector2)
      nueva_frase.append(revisa_frase)
      similitud.append(similitud_j)
      vector.append(vector2)
    frases_similitud = pd.Series(data=similitud, index=nueva_frase)
    frases_vector = pd.Series(data=vector, index=nueva_frase)
    max_sim = frases_similitud.idxmax()
    vector1 = frases_vector[frases_similitud.idxmax()]
    Nuevo_texto.append(max_sim)
    del similitud_j, nueva_frase
    print('Oración: ',n+2,' de ',N,' ...')

file = open(path_doc + "discurso.txt", "w")
print('\n====== Nuevo Discurso ======\n')
for oraciones in Nuevo_texto:
    oraciones = oraciones.replace(u'\n','')
    print(oraciones,"\n")
    file.write(oraciones + os.linesep)
file.close()